import pandas as pd
from tqdm import tqdm
import chardet  # è‡ªåŠ¨æ£€æµ‹ç¼–ç 

# ======= æ–‡ä»¶è·¯å¾„è®¾ç½® =======
feature_file = "NUSW-NB15_features.csv"   # ç‰¹å¾è¯´æ˜æ–‡ä»¶
data_file = "UNSW_NB15_training-set.csv"               # æ•°æ®é›†æ–‡ä»¶

# ======= 1ï¸âƒ£ è‡ªåŠ¨æ£€æµ‹ç‰¹å¾è¯´æ˜æ–‡ä»¶ç¼–ç  =======
with open(feature_file, "rb") as f:
    detect_result = chardet.detect(f.read(2048))
encoding = detect_result["encoding"]
print(f"ğŸ“˜ æ£€æµ‹åˆ°ç‰¹å¾è¯´æ˜æ–‡ä»¶ç¼–ç ï¼š{encoding}")

# é‡æ–°è¯»å–æ–‡ä»¶
try:
    features = pd.read_csv(feature_file, encoding=encoding)
except Exception as e:
    print("âŒ æ— æ³•è¯»å–ç‰¹å¾è¯´æ˜æ–‡ä»¶ï¼š", e)
    exit()

expected_cols = {"No.", "Name", "Type", "Description"}

if not expected_cols.issubset(features.columns):
    print("âš ï¸ ç‰¹å¾è¯´æ˜æ–‡ä»¶çš„åˆ—åä¸é¢„æœŸä¸ç¬¦ï¼Œå®é™…åˆ—åä¸ºï¼š", list(features.columns))
    exit()

# ======= æ ¹æ® Type åˆ—åˆ¤æ–­å˜é‡ç±»å‹ =======
type_map = {
    "binary": "0/1 äºŒåˆ†å˜é‡",
    "nominal": "åˆ†ç±»å˜é‡ (é0/1)",
    "integer": "æ•°å€¼å˜é‡",
    "float": "æ•°å€¼å˜é‡",
    "numeric": "æ•°å€¼å˜é‡"
}
features["å˜é‡ç±»åˆ«"] = features["Type"].apply(
    lambda x: type_map.get(str(x).lower(), "å…¶ä»–ç±»å‹")
)

binary_vars = features.loc[features["å˜é‡ç±»åˆ«"] == "0/1 äºŒåˆ†å˜é‡", "Name"].tolist()

print(f"âœ… è¯†åˆ«åˆ° {len(binary_vars)} ä¸ª 0/1 äºŒåˆ†å˜é‡ï¼š")
print(binary_vars)

# ======= 2ï¸âƒ£ è¯»å–çœŸå®æ•°æ®é›† =======
try:
    df = pd.read_csv(data_file)
except Exception as e:
    print("âŒ æ— æ³•è¯»å–æ•°æ®é›†æ–‡ä»¶ï¼š", e)
    exit()

print(f"\nğŸ“Š åŸå§‹æ•°æ®é›†ï¼š{df.shape[0]} è¡Œ Ã— {df.shape[1]} åˆ—")

# ======= 3ï¸âƒ£ æ¸…ç†éæ³• 0/1 å€¼ï¼ˆå¸¦è¿›åº¦æ¡ï¼‰ =======
invalid_rows = 0
print("\nğŸš§ æ­£åœ¨æ£€æŸ¥å¹¶åˆ é™¤ 0/1 å˜é‡ä¸­éæ³•å–å€¼...")
for col in tqdm(binary_vars, desc="æ£€æŸ¥0/1å˜é‡", ncols=80):
    if col in df.columns:
        invalid_mask = ~df[col].isin([0, 1])
        count_invalid = invalid_mask.sum()
        if count_invalid > 0:
            invalid_rows += count_invalid
            df = df[~invalid_mask]

if invalid_rows == 0:
    print("âœ… æ‰€æœ‰0/1å˜é‡çš„å€¼å‡åˆæ³•ã€‚")
else:
    print(f"âš ï¸ å…±åˆ é™¤ {invalid_rows} è¡ŒåŒ…å«éæ³•0/1å€¼çš„æ•°æ®ã€‚")

# ======= 4ï¸âƒ£ å»é™¤ç‰¹å¾å†²çªè¡Œï¼ˆå¿½ç•¥ id åˆ—ï¼‰ =======
print("\nğŸš§ æ­£åœ¨æ£€æŸ¥å¹¶åˆ é™¤ç‰¹å¾å†²çªè¡Œï¼ˆç‰¹å¾ç›¸åŒä½† label æˆ– attack_cat ä¸åŒï¼‰...")

# å¿½ç•¥ idã€labelã€attack_cat ä¸‰ä¸ªåˆ—
ignore_cols = ["id", "attack_cat", "label"]
non_label_cols = [c for c in df.columns if c not in ignore_cols]

# åˆ†ç»„æ£€æµ‹
conflict_idx = []

# tqdm ä¸ç›´æ¥æ”¯æŒ groupbyï¼Œè¿™é‡Œæ˜¾ç¤ºæ€»ä½“è¿›åº¦æç¤º
grouped = df.groupby(non_label_cols, dropna=False)
for _, group in tqdm(grouped, total=len(grouped), desc="æ£€æŸ¥å†²çª", ncols=80):
    # è‹¥åŒä¸€ç»„ä¸­ label æˆ– attack_cat æœ‰å¤šä¸ªä¸åŒå€¼ â†’ å†²çª
    if len(group[["attack_cat", "label"]].drop_duplicates()) > 1:
        conflict_idx.extend(group.index)

# åˆ é™¤å†²çªè¡Œ
if conflict_idx:
    conflict_count = len(conflict_idx)
    df = df.drop(conflict_idx)
    print(f"âš ï¸ æ£€æµ‹åˆ° {conflict_count} è¡Œç‰¹å¾å†²çªæ•°æ®ï¼ˆå¿½ç•¥ idï¼‰ï¼Œå·²åˆ é™¤ã€‚")
else:
    print("âœ… æœªæ£€æµ‹åˆ°ç‰¹å¾å†²çªè¡Œã€‚")

# ======= 5ï¸âƒ£ è¾“å‡ºç»Ÿè®¡ä¸ä¿å­˜ =======
print(f"\nğŸ“Š æ¸…ç†åæ•°æ®é›†ï¼š{df.shape[0]} è¡Œ Ã— {df.shape[1]} åˆ—")

output_file = "UNSW-NB15_cleaned_tr.csv"
df.to_csv(output_file, index=False, encoding="utf-8-sig")
print(f"âœ… å·²ä¿å­˜æ¸…ç†åçš„æ•°æ®é›†ï¼š{output_file}")

# ======= é™„åŠ ï¼šè¾“å‡ºç±»å‹ç»Ÿè®¡ =======
print("\n===== ç‰¹å¾ç±»å‹ç»Ÿè®¡ =====")
print(features["å˜é‡ç±»åˆ«"].value_counts())
